<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="HMM, Python, Viterbi," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="说明  本文代码对应Graham Neubig的nlptutorial系列中的第四讲Part of Speech Tagging with Hidden Markov Models，主要讲解用隐马尔科夫模型（Hidden Markov Model，HMM）实现一个词性标注器。 对应Slides位于此处。 本文的示例代码位于此处。 依赖  Python3 numpy  建议在阅读代码（5 HMM 训">
<meta name="keywords" content="HMM, Python, Viterbi">
<meta property="og:type" content="article">
<meta property="og:title" content="基于HMM模型的词性标注器Python实现">
<meta property="og:url" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="说明  本文代码对应Graham Neubig的nlptutorial系列中的第四讲Part of Speech Tagging with Hidden Markov Models，主要讲解用隐马尔科夫模型（Hidden Markov Model，HMM）实现一个词性标注器。 对应Slides位于此处。 本文的示例代码位于此处。 依赖  Python3 numpy  建议在阅读代码（5 HMM 训">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/hmm_1.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/hmm_2.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/dp_1.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/dp_2.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/best_path.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/lattice.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/forward_first.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/forward_middle.png">
<meta property="og:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/forward_final.png">
<meta property="og:updated_time" content="2017-10-23T08:50:37.285Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于HMM模型的词性标注器Python实现">
<meta name="twitter:description" content="说明  本文代码对应Graham Neubig的nlptutorial系列中的第四讲Part of Speech Tagging with Hidden Markov Models，主要讲解用隐马尔科夫模型（Hidden Markov Model，HMM）实现一个词性标注器。 对应Slides位于此处。 本文的示例代码位于此处。 依赖  Python3 numpy  建议在阅读代码（5 HMM 训">
<meta name="twitter:image" content="http://yoursite.com/2017/10/23/hmm-pos-tagger/hmm_1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/10/23/hmm-pos-tagger/"/>





  <title>基于HMM模型的词性标注器Python实现 | Hexo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/23/hmm-pos-tagger/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">基于HMM模型的词性标注器Python实现</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-10-23T15:00:56+08:00">
                2017-10-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="说明">说明</h2>
<ul>
<li><p>本文代码对应Graham Neubig的<a href="https://github.com/neubig/nlptutorial" target="_blank" rel="external">nlptutorial</a>系列中的第四讲<strong>Part of Speech Tagging with Hidden Markov Models</strong>，主要讲解用<strong>隐马尔科夫模型（Hidden Markov Model，HMM）</strong>实现一个<strong>词性标注器</strong>。</p></li>
<li><p>对应Slides位于<a href="https://github.com/neubig/nlptutorial/tree/master/download/04-hmm" target="_blank" rel="external">此处</a>。</p></li>
<li><p>本文的示例代码位于<a href="https://github.com/psubnwell/nlptutorial-exercise/tree/master/exercise/04-hmm" target="_blank" rel="external">此处</a>。</p></li>
<li>依赖
<ul>
<li><code>Python3</code></li>
<li><code>numpy</code></li>
</ul></li>
<li><p>建议在阅读代码（<code>5 HMM 训练</code>~<code>8 HMM 测试（维特比算法）</code>小节）之前，至少需要过一遍简单的真实例子（<code>4 HMM 真实例子</code>小节），知道代码在干什么。也建议快速读一下基础部分（<code>1 HMM 基础</code>和<code>2 生成式模型基础</code>），该部分既不严谨也不全面，但对于阅读代码而言足够。</p></li>
</ul>
<h2 id="生成式模型基础">生成式模型基础</h2>
<ul>
<li><p><span class="math inline">\(x = x_1, x_2, ..., x_n\)</span> 观察变量，本文中可以视为单词序列。</p></li>
<li><p><span class="math inline">\(y = y_1, y_2, ..., y_n\)</span> 隐变量（状态变量），本文中可以视为词性序列。</p></li>
<li><p>问题：给定<span class="math inline">\(x\)</span>，如何求各种<span class="math inline">\(y\)</span>的可能性（即<span class="math inline">\(P(y|x)\)</span>）？或者说，如何求最大可能性的<span class="math inline">\(y\)</span>（即<span class="math inline">\(y=argmax_y P(y|x)\)</span>）？</p></li>
<li>有两种方法：
<ul>
<li>一种方法是直接对<span class="math inline">\(P(y|x)\)</span>建模。叫做<strong>判别式模型（discrimitive model）</strong>；</li>
<li>另一种方法则迂回一下，将<span class="math inline">\(P(y|x)\)</span>转化为<span class="math inline">\(P(y|x)=\frac{P(x, y)}{P(x)}\)</span>来求解。又因为<span class="math inline">\(P(x, y)\)</span>一般特别难以完全求解，因为 <span class="math display">\[
\begin{align*}
P(x, y) &amp;= P(x_1, x_2, ..., x_n, y_1, y_2, ..., y_n) \\
&amp;= P(x_1)\cdot P(x_2|x_1)\cdot P(x_3|x_1, x_2)\cdot...\cdot  \\
&amp; P(y_1|x_1, x_2, ..., x_n)\cdot P(y_2|x_1, x_2, ..., x_n, y_1)\cdot...\cdot \\
&amp; P(y_n|x_1, x_2, ..., x_n, y_1, y_2, ..., y_{n-1})
\end{align*}
\]</span> HMM模型和其他一些生成式模型都是在想方设法简化该项的分解形式。一般进一步转化为 <span class="math inline">\(P(y|x)=\frac{P(x|y)P(y)}{P(x)}\)</span>。叫做<strong>生成式模型（generative model）</strong>。之所以叫这个名字，大概是因为这个公式中含有<span class="math inline">\(P(x|y)\)</span>，这个意思是给定标签“生成”数据。一般来说标签数量较少（比如只有几十种词性），而数据是千变万化的（比如有成千上万的单词），“少”的标签去生成“多”的数据，是谓<strong>生成</strong>。</li>
</ul></li>
<li><p>大部分情况下我们是已知<span class="math inline">\(x\)</span>，只需求<strong>最有可能</strong>的<span class="math inline">\(y\)</span>，而无需求<strong>所有可能</strong>的<span class="math inline">\(y\)</span>的分布。即：<span class="math display">\[y=argmax_y P(y|x) = argmax_y \frac{P(x|y)P(y)}{P(x)}\]</span> 因为分母和所求参数<span class="math inline">\(y\)</span>无关，因此可以不用求<span class="math inline">\(P(x)\)</span>，省去大量计算。即： <span class="math display">\[y=argmax_y P(y|x) = argmax_y P(x|y)P(y)\]</span> （注：这种方法有个术语。在贝叶斯学派的观念中，参数和变量没有区别，所以可以将变量<span class="math inline">\(y\)</span>视为参数来做估计。这里描述的这种不求完整分布，只需要求能达到最大概率的参数值的方法，在参数估计中叫做<strong>最大后验概率估计（Maximum A Posteriori，MAP）</strong>。）</p></li>
</ul>
<h2 id="hmm-基础">HMM 基础</h2>
<ul>
<li><p>HMM将公式<span class="math inline">\(y=argmax_y P(y|x) = argmax_y P(x|y)P(y)\)</span>进一步拆解。</p>
<div class="figure">
<img src="/2017/10/23/hmm-pos-tagger/hmm_1.png">

</div>
<p>上图中一个灰色椭圆代表一个条件概率。</p></li>
<li><p>通过<strong>独立输出假设</strong>：每一个观测变量状态跟且仅跟其隐变量状态相关。简化下式： <span class="math display">\[P(x|y)=P(x_1,x_2,...,x_n|y_1, y_2, ..., y_n)=P(x_1|y_1)P(x_2|y_2)...P(x_n|y_n)\]</span></p></li>
<li><p>通过<strong>马尔可夫假设</strong>：后一状态只与前一状态有关。简化下式： <span class="math display">\[P(y)=P(y_1, y_2, ..., y_n)=P(y_1|&lt;s&gt;)P(y_2|y_1)...P(&lt;/s&gt;|y_n)\]</span> （<code>&lt;s&gt;</code>和<code>&lt;/s&gt;</code>表示句子首尾）</p></li>
<li><p>所以： <span class="math display">\[
\begin{align*}
y &amp;= argmax_y P(y|x) = argmax_y P(x|y)P(y) \\
&amp;= argmax_{y_1, y_2, ..., y_n} \prod_i^n P(x_i | y_i) \prod_i^n P(y_i|y_{i-1})
\end{align*}
\]</span></p></li>
</ul>
<h2 id="hmm-真实例子">HMM 真实例子</h2>
<ul>
<li><p><span class="math inline">\(x = 你, 很, 美\)</span></p></li>
<li><p><span class="math inline">\(y = 代词, 副词, 形容词\)</span> （可能的词性序列之一）</p></li>
<li><p>HMM 的图示如下：</p>
<div class="figure">
<img src="/2017/10/23/hmm-pos-tagger/hmm_2.png">

</div></li>
<li><p>我们问题是：已知<span class="math inline">\(x = 你, 很, 美\)</span>，计算该序列对应的词性序列为<span class="math inline">\(y = 代词, 副词, 形容词\)</span>的概率。</p></li>
<li><p>由前面的计算公式可知： <span class="math display">\[
\begin{align*}
y &amp;= argmax_y P(y|x) = argmax_y \frac{P(x, y)}{P(x)} \\
&amp;= argmax_y P(x, y) = argmax_y P(x|y)P(y) \\
&amp;= argmax_{y_1, y_2, ..., y_n} \prod_i^n P(x_i | y_i) \prod_i^n P(y_i|y_{i-1})
\end{align*}
\]</span> 这里<span class="math inline">\(y\)</span>有多种可能，这里以<span class="math inline">\(y = 代词, 副词, 形容词\)</span>为例，计算<span class="math inline">\(argmax\)</span>内部的<span class="math inline">\(P(x, y)\)</span>： <span class="math display">\[
\begin{align*}
P(x, y) &amp;= P(你, 很, 美, &lt;s&gt;, 代词, 副词, 形容词, &lt;/s&gt;) \\
&amp;= P(你, 很, 美|&lt;s&gt;, 代词, 副词, 形容词, &lt;/s&gt;) \cdot \\
&amp; P(&lt;s&gt;, 代词, 副词, 形容词, &lt;/s&gt;) \\
&amp;= [P(你|代词)P(很|副词)P(美|形容词)]\cdot \\
&amp; [P(代词|&lt;s&gt;)P(副词|代词)P(形容词|副词)P(&lt;/s&gt;|形容词)]
\end{align*}
\]</span> 我们要求出每种可能的<span class="math inline">\(y\)</span>序列的<span class="math inline">\(P(x, y)\)</span>值，然后取让其最大的<span class="math inline">\(y\)</span>序列（即<span class="math inline">\(argmax_{y_1, y_2, ..., y_n}\)</span>）。 在按照这样计算每一个<span class="math inline">\(y\)</span>序列之后，取值最大的，返回对应的<span class="math inline">\(y\)</span>序列，就是我们认为正确的词性。</p></li>
<li><p>“<span class="math inline">\(P(你|代词)P(很|副词)P(美|形容词)\)</span>”叫做<strong>发射概率（emission probability）</strong>，字面意思是从一个词性中<strong>发射/生成</strong>出某一个单词的概率。比如<span class="math inline">\(P(你|代词)\)</span>表示从这么多代词中选一个对应的单词，这个单词为“你”的可能性是多少。</p></li>
<li><p>“<span class="math inline">\(P(代词|&lt;s&gt;)P(副词|代词)P(形容词|副词)P(&lt;/s&gt;|形容词)\)</span>”叫做<strong>转移概率（transition probability）</strong>，表示从一个词性转移到下一个词性的概率。比如<span class="math inline">\(P(副词|代词)\)</span>表示上一个词的词性是代词，那么下一个词的词性是副词的概率是多少。</p></li>
<li><p>发射概率和转移概率都可以从标注文档中通过统计得到。</p></li>
<li><p>HMM 主要分为两部分：训练和解码。分别对应本文档中的<code>5 HMM 训练</code>和<code>8 HMM 测试（维特比算法）</code>。对应代码中的<code>train_hmm.py</code>和<code>test_hmm.py</code>。</p></li>
<li><p><strong>最后要注意一点是：上面的概率有的值很小，相乘后就更小了，限于计算机的精度，相乘容易造成下溢。一般我们会使用log来将相乘变为相加。</strong>如果使用对数（log）进行处理，则依旧是求<span class="math inline">\(argmax\)</span>；如果使用负对数（-log）进行处理，则改为求<span class="math inline">\(argmin\)</span>。Neubig的Slides中取了负对数，因此我们实验中也取负对数。</p></li>
</ul>
<h2 id="hmm-训练">HMM 训练</h2>
<ul>
<li><p><code>train_hmm.py</code>的 API 和 main 函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def train_hmm(training_file, model_file):  # 总函数。</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    parser = argparse.ArgumentParser()  # 定义命令行传入参数。</div><div class="line">    parser.add_argument(&apos;--training-file&apos;, type=str)</div><div class="line">    parser.add_argument(&apos;--model-file&apos;, type=str, default=&apos;stdout&apos;)</div><div class="line">    args = parser.parse_args()</div><div class="line"></div><div class="line">    train_hmm(args.training_file, args.model_file)</div></pre></td></tr></table></figure></p></li>
<li><p>输入文件（标注语料）： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ less data/wiki-en-train.norm_pos</div><div class="line">Natural_JJ language_NN processing_NN -LRB-_-LRB- NLP_NN -RRB-_-RRB- is_VBZ a_DT field_NN of_IN computer_NN science_NN ,_, artificial_JJ intelligence_NN -LRB-_-LRB- also_RB called_VBN machine_NN learning_NN -RRB-_-RRB- ,_, and_CC linguistics_NNS concerned_VBN with_IN the_DT interactions_NNS between_IN computers_NNS and_CC human_JJ -LRB-_-LRB- natural_JJ -RRB-_-RRB- languages_NNS ._.</div><div class="line">...</div></pre></td></tr></table></figure></p></li>
<li><code>train_hmm()</code>主要做了两件事：
<ul>
<li>统计<strong>转移概率（transition probability）</strong>，即连续的两个tag出现的频次/频率，对应HMM模型中的<span class="math inline">\(p(y_i|y_{i-1})\)</span>。</li>
<li>统计<strong>发射概率（emission probability）</strong>，即某一个tag生成某一个word的频次/频率，对应HMM模型中的<span class="math inline">\(p(x_i|y_i)\)</span>。</li>
</ul></li>
<li><p>Demo <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python3 exercise/04-hmm/train_hmm.py --training-file=&apos;data/wiki-en-train.norm_pos&apos; --model-file=&apos;exercise/04-hmm/my_model&apos;</div></pre></td></tr></table></figure></p></li>
<li><p>输出文件（模型）： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ less exercise/04-hmm/my_model</div><div class="line">T DT NN 0.5120500782472613 # 以T开头的为转移概率</div><div class="line">T JJ NN 0.4236357765579593 # 表示上一个词性是JJ，则下一个词性是NN的概率为0.424。</div><div class="line">T IN DT 0.33908496732026144</div><div class="line">T NN IN 0.20562700964630226</div><div class="line">T . &lt;/s&gt; 0.9785768936495792</div><div class="line">...</div><div class="line">E NNP Learning 0.000727802037845706 # 以E开头的为发射概率</div><div class="line">E NN markup 0.0001607717041800643 # 表示从所有词性为NN的单词中选到markup的概率是0.00016。</div><div class="line">E NNP HTML 0.000727802037845706</div><div class="line">E NNS formats 0.0003832886163280951</div><div class="line">E NN PDF 0.0001607717041800643</div></pre></td></tr></table></figure></p></li>
</ul>
<h2 id="hmm-随机采样">HMM 随机采样</h2>
<ul>
<li><p>初始化一个隐变量（词性），让马尔可夫链自动随机生成可观测变量（词）和下一个隐变量（词性），直至遇到<code>EOS</code>为止。</p></li>
<li><p><code>random_sample.py</code> 该部分不再展示过多细节。核心函数numpy已经实现好了： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">output_word = np.random.choice(candidate_word, p=candidate_word_prob)</div><div class="line">next_prob = np.random.choice(candidate_tag, p=candidate_tag_prob)</div></pre></td></tr></table></figure></p>
<p>第一行代码的意思是：（已知一个词性，）在一系列候选单词中，按照它们的分布概率（发射概率），随机选取一个单词作为可观测变量输出。 第二行代码的意思类似：（已知前一个词性，）在一系列候选词性中，按照它门的分布（迁移概率），随机选取一个词性作为下一个词性输出。</p></li>
<li><p>Demo： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python3 exercise/04-hmm/random_sample.py --model-file=&apos;exercise/04-hmm/my_model&apos;</div></pre></td></tr></table></figure></p>
<p>多次重复可以得到不同的结果： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">all the movie .</div><div class="line">how a words alone novel case is those gradual discourse or It is attaching This analyses .</div><div class="line">Journal &apos;s Question Cynthia .</div><div class="line">simplest range segmentation scripts .</div></pre></td></tr></table></figure></p>
<p>注意，在我们观察输出的搞笑句子时，要想象这个句子由一个隐藏的词性链条在驱动着、生成着。</p></li>
</ul>
<h2 id="动态规划思想">动态规划思想</h2>
<ul>
<li><p><strong>如果从最佳选择（最短、概率最大、负对数概率最小）的路径的末端截取一小部分，余下的路径仍然是最佳路径。</strong></p></li>
<li><p>特别关注一点：<strong>路径中的状态交点</strong>。</p>
<p><img src="/2017/10/23/hmm-pos-tagger/dp_1.png"> <img src="/2017/10/23/hmm-pos-tagger/dp_2.png"></p></li>
<li><p>动态规划可以解决任何一个图中的最短路径问题。</p>
<div class="figure">
<img src="/2017/10/23/hmm-pos-tagger/best_path.png">

</div></li>
<li>Q：怎么实现DP？ A：两种常用的方法。（仅仅涉及一般问题）
<ul>
<li><strong>自下而上</strong>：通过正向的loop，求出所有状态对应的值，然后找出max或者min。 优缺点：速度慢，但是相对节省空间。（本次实验展示的方法。）</li>
<li><strong>自上而下</strong>：通过递归的方法，需要求解<span class="math inline">\(f(x)\)</span>，则必须知道<span class="math inline">\(f(y)\)</span>，要知道<span class="math inline">\(f(y)\)</span>，必须求<span class="math inline">\(f(z)\)</span>。优缺点：速度快，只用算需要的值，但是要调用堆栈，浪费空间。（<strong>在这种思想中，动态规划实际上是对递归的优化（递归2.0），防止了不必要的递归（递归树中有相同分支）</strong>）。</li>
</ul></li>
</ul>
<h2 id="hmm-测试维特比算法">HMM 测试（维特比算法）</h2>
<ul>
<li><p>维特比算法是一个特殊但应用最广的动态规划算法，专门针对一个特殊的图——<strong>篱笆网络（Lattice）</strong>有向图。凡使用HMM描述的问题都可以用它来解码。</p>
<div class="figure">
<img src="/2017/10/23/hmm-pos-tagger/lattice.png">

</div></li>
<li>上图中，每一个单词下方都有一串<strong>候选词性</strong>（句子开始和结尾处的候选词性为<code>&lt;s&gt;</code>和<code>&lt;/s&gt;</code>）。维特比算法中，
<ul>
<li>前向部分，我们的任务是计算出<strong>从“初始状态节点（图中的<code>0:&lt;s&gt;</code>）”到每一步中每一个候选隐状态节点（图中的大量灰盒子，如<code>2:NN</code>）的最短路径和对应的概率值</strong>。</li>
<li>反向部分，根据前面的结果得到一条完整的最佳路径。</li>
</ul></li>
<li><p>显然，在每两步之间（图中第<span class="math inline">\(i\)</span>列和第<span class="math inline">\(i\)</span>列）时，根据动态规划思想，我们已经知道、且无需再考虑之前第<span class="math inline">\(0\)</span>列到第<span class="math inline">\(i-1\)</span>列的最佳路径，直接拿来用即可。我们需要做的仅仅是采用两个for循环来对第<span class="math inline">\(i\)</span>列和第<span class="math inline">\(i\)</span>列的候选状态节点进行遍历即可。因此动态规划的复杂度为<span class="math inline">\(O(mn^2)\)</span>（m是序列长度，n是每一层的候选隐状态个数）。</p></li>
<li><p><code>test_hmm.py</code>的API和 main 函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">def load_model(model_file):  # 用于加载模型。</div><div class="line">def prob_trans(key, model):  # 用于从模型中返回转移概率。</div><div class="line">def prob_emiss(key, model):  # 用于从模型中返回发射概率。</div><div class="line">def forward(transition, emission, possible_tags, line):  # Viterbi算法的正向算法。</div><div class="line">def backward(best_edge, line):  # Viterbi算法的反向算法。</div><div class="line">def test_hmm(model_file, test_file, output_file):  # 总函数。</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    parser = argparse.ArgumentParser()  # 定义命令行传入参数。</div><div class="line">    parser.add_argument(&apos;--model-file&apos;, type=str)</div><div class="line">    parser.add_argument(&apos;--test-file&apos;, type=str)</div><div class="line">    parser.add_argument(&apos;--output-file&apos;, type=str, default=&apos;stdout&apos;)</div><div class="line">    args = parser.parse_args()</div><div class="line"></div><div class="line">    test_hmm(args.model_file, args.test_file, args.output_file)</div></pre></td></tr></table></figure></p></li>
<li><strong>前向算法（First Part）</strong>
<ul>
<li><strong>使用log是因为原来的概率值相乘，有的数值过小，限于计算机的精度，相乘容易造成下溢。</strong></li>
<li>下图中，<code>best_score['1 NN']</code>表示从初始节点（<code>'0 &lt;s&gt;'</code>）到该节点（‘1 NN’）的最优路线的值（负对数）。</li>
<li><code>best_edge['1 NN'] = '0 &lt;s&gt;'</code>（图中没有显示）表示从初始节点（<code>'0 &lt;s&gt;'</code>）到该节点（‘1 NN’）的最优路线中前一个节点是<code>'0 &lt;s&gt;'</code>。</li>
</ul>
<div class="figure">
<img src="/2017/10/23/hmm-pos-tagger/forward_first.png">

</div>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># First part, described in Neubig&apos;s slides p.38.</div><div class="line">for next in possible_tags.keys():</div><div class="line">    for prev in [SOS]:</div><div class="line">        prev_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(0, prev)</div><div class="line">        next_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(1, next)</div><div class="line">        trans_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(prev, next)</div><div class="line">        emiss_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(next, words[0])</div><div class="line">        if prev_key in best_score and trans_key in transition:</div><div class="line">            score = best_score[prev_key] + \</div><div class="line">                    -math.log2(prob_trans(trans_key, transition)) + \</div><div class="line">                    -math.log2(prob_emiss(emiss_key, emission))</div><div class="line">            if next_key not in best_score or best_score[next_key] &gt; score:</div><div class="line">                best_score[next_key] = score</div><div class="line">                best_edge[next_key] = prev_key</div></pre></td></tr></table></figure></li>
<li><strong>前向算法（Middle Part）</strong>
<ul>
<li><code>best_score['2 NN']</code>表示从初始节点（<code>'0 &lt;s&gt;'</code>）到该节点（‘2 NN’）的最优路线的值（负对数）。因为要取最优，显然<strong>本层每个单元</strong>要对<strong>前一层的所有单元</strong>进行一次计算，然后取最小值。<strong>所以代码中有两个for循环。所以算法复杂度为O(nm^2)。</strong></li>
<li><code>best_edge['2 NN'] = '1 JJ'</code>（打个比方，图中没有显示）表示从初始节点（<code>'0 &lt;s&gt;'</code>）到该节点（‘2 NN’）的最优路线中前一个节点是<code>'1 JJ'</code>。</li>
</ul>
<div class="figure">
<img src="/2017/10/23/hmm-pos-tagger/forward_middle.png">

</div>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># Middle part, described in Neubig&apos;s slides p.39.</div><div class="line">for i in range(1, l):</div><div class="line">    for next in possible_tags.keys():</div><div class="line">        for prev in possible_tags.keys():</div><div class="line">            prev_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(i, prev)</div><div class="line">            next_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(i + 1, next)</div><div class="line">            trans_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(prev, next)</div><div class="line">            emiss_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(next, words[i])</div><div class="line">            if prev_key in best_score and trans_key in transition:</div><div class="line">                score = best_score[prev_key] + \</div><div class="line">                        -math.log2(prob_trans(trans_key, transition)) + \</div><div class="line">                        -math.log2(prob_emiss(emiss_key, emission))</div><div class="line">                if next_key not in best_score or best_score[next_key] &gt; score:</div><div class="line">                    best_score[next_key] = score</div><div class="line">                    best_edge[next_key] = prev_key</div></pre></td></tr></table></figure></li>
<li><p><strong>前向算法（Final Part）</strong></p>
<div class="figure">
<img src="/2017/10/23/hmm-pos-tagger/forward_final.png">

</div>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># Final part, described in Neubig&apos;s slides p.40.</div><div class="line">for next in [EOS]:</div><div class="line">    for prev in possible_tags.keys():</div><div class="line">        prev_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(l, prev)</div><div class="line">        next_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(l + 1, next)</div><div class="line">        trans_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(prev, next)</div><div class="line">        emiss_key = &apos;&#123;&#125; &#123;&#125;&apos;.format(next, EOS)</div><div class="line">        if prev_key in best_score and trans_key in transition:</div><div class="line">            score = best_score[prev_key] + \</div><div class="line">                    -math.log2(prob_trans(trans_key, transition))</div><div class="line">            if next_key not in best_score or best_score[next_key] &gt; score:</div><div class="line">                best_score[next_key] = score</div><div class="line">                best_edge[next_key] = prev_key</div></pre></td></tr></table></figure></li>
<li><strong>反向算法</strong>
<ul>
<li>反向算法即按照前向函数<code>forward()</code>返回的<code>best_edge</code>，从尾到首走一遍即可。</li>
</ul></li>
<li><p><strong>Demo</strong></p>
<p>测试语料内容为： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ less data/wiki-en-test.norm</div><div class="line">In computational linguistics , word-sense disambiguation -LRB- WSD -RRB- is an open problem of natural language processing , which governs the process of identifying which sense of a word -LRB- i.e. meaning -RRB- is used in a sentence , when the word has multiple meanings -LRB- polysemy -RRB- .</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>运行程序： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python3 exercise/04-hmm/test_hmm.py --model-file=&apos;exercise/04-hmm/my_model&apos; --test-file=&apos;data/wiki-en-test.norm&apos; --output-file=&apos;exercise/04-hmm/my_answer.pos&apos;</div></pre></td></tr></table></figure></p>
<p>一般这个程序会跑10s-20s左右。跑得结果： <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ less exercise/04-hmm/my_answer.pos</div><div class="line">IN JJ NNS , DT NN -LRB- NN -RRB- VBZ DT JJ NN IN JJ NN NN , WDT VBZ DT NN IN VBG DT NN IN DT NN -LRB- FW NN -RRB- VBZ VBN IN DT NN , WRB DT NN VBZ JJ NNS -LRB- NN -RRB- .</div><div class="line">...</div></pre></td></tr></table></figure></p></li>
<li><p>评测性能 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ perl script/gradepos.pl data/wiki-en-test.pos exercise/04-hmm/my_answer.pos</div></pre></td></tr></table></figure></p>
<p>如果没错的话，你应该看到90.82%的正确率！ <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Accuracy: 90.82% (4144/4563)</div><div class="line"></div><div class="line">Most common mistakes:</div><div class="line">NNS --&gt; NN	45</div><div class="line">NN --&gt; JJ	27</div><div class="line">NNP --&gt; NN	22</div><div class="line">JJ --&gt; DT	22</div><div class="line">VBN --&gt; NN	12</div><div class="line">JJ --&gt; NN	12</div><div class="line">NN --&gt; IN	11</div><div class="line">NN --&gt; DT	10</div><div class="line">NNP --&gt; JJ	8</div><div class="line">JJ --&gt; VBN	7</div></pre></td></tr></table></figure></p></li>
</ul>
<h2 id="参考文献">参考文献</h2>
<ol style="list-style-type: decimal">
<li><a href="https://github.com/neubig/nlptutorial/tree/master/download/04-hmm" target="_blank" rel="external">Slides of 04-hmm</a> of <a href="https://github.com/neubig/nlptutorial" target="_blank" rel="external">nlptutorial</a>, by Graham Neubig</li>
<li><a href="http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf" target="_blank" rel="external">Lecture Notes</a> of <a href="http://www.cs.columbia.edu/~mcollins/notes-spring2013.html" target="_blank" rel="external">Coursera: Natural Language Process</a>, by Michael Collins</li>
<li>机器学习, 周志华 著</li>
<li><a href="https://www.zhihu.com/question/39948290" target="_blank" rel="external">如何理解动态规划？</a>, 知乎</li>
<li>数学之美, 吴军 著</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/HMM-Python-Viterbi/" rel="tag"># HMM, Python, Viterbi</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/14/test-my-site/" rel="next" title="test_my_site">
                <i class="fa fa-chevron-left"></i> test_my_site
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Inhaltsverzeichnis
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Übersicht
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <p class="site-author-name" itemprop="name">John Doe</p>
            <p class="site-description motion-element" itemprop="description"></p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">Artikel</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">Tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#说明"><span class="nav-number">1.</span> <span class="nav-text">说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#生成式模型基础"><span class="nav-number">2.</span> <span class="nav-text">生成式模型基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hmm-基础"><span class="nav-number">3.</span> <span class="nav-text">HMM 基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hmm-真实例子"><span class="nav-number">4.</span> <span class="nav-text">HMM 真实例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hmm-训练"><span class="nav-number">5.</span> <span class="nav-text">HMM 训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hmm-随机采样"><span class="nav-number">6.</span> <span class="nav-text">HMM 随机采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#动态规划思想"><span class="nav-number">7.</span> <span class="nav-text">动态规划思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hmm-测试维特比算法"><span class="nav-number">8.</span> <span class="nav-text">HMM 测试（维特比算法）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">9.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
